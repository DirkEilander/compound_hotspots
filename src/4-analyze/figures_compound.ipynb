{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from plot_tools import *\n",
    "import cartopy.crs as ccrs\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0, t1 = datetime(1980,1,1), datetime(2014,12,30)\n",
    "\n",
    "# criteria\n",
    "q = 95\n",
    "min_dist = 45\n",
    "window_size = 7\n",
    "qmin=0\n",
    "rp = 20\n",
    "\n",
    "model = 'mean' #anu\n",
    "drivers = ['runoff', 'surge', 'cmpnd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddir = r'/scratch/compound'\n",
    "fig_dir = r'/scratch/compound/figs'\n",
    "\n",
    "fn_peaks = join(ddir, f'peaks_q{q}d{min_dist}_wdw{window_size}.nc')\n",
    "ds_peaks = xr.open_dataset(fn_peaks, chunks={'ensemble':-1, 'index':250, 'time': -1})\n",
    "ds_peaks.ensemble.data = ds_peaks.ensemble.data.astype(str)\n",
    "\n",
    "fn_peaks_stats = join(ddir, f'peaks_q{q}d{min_dist}_wdw{window_size}_stats.nc')\n",
    "ds_peaks_stats = xr.open_dataset(fn_peaks_stats, chunks={'ensemble':-1, 'index':250})\n",
    "ds_peaks_stats.ensemble.data = ds_peaks_stats.ensemble.data.astype(str)\n",
    "\n",
    "fn_cmf = join(ddir, 'gcfr.zarr')\n",
    "ds_cmf = xr.open_zarr(fn_cmf).sel(time=slice(t0,t1))\n",
    "ds_cmf.ensemble.data = ds_cmf.ensemble.data.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce ensemble dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_var = [v for v in ds_peaks_stats.data_vars if ds_peaks_stats[v].ndim >= 3] + ['rp', 'scen']\n",
    "if model == 'mean':\n",
    "    ds_peaks_stats_mod = ds_peaks_stats.sel(rp=rp).drop(drop_var).mean('ensemble')\n",
    "    peaks_all_n = ds_peaks_stats[f'peaks100_n'].sum('ensemble')\n",
    "    for driver in drivers:\n",
    "        peaks_driver_n = ds_peaks_stats[f'peaks_{driver}_n'].sum('ensemble')\n",
    "        # ensemble mean\n",
    "        ds_peaks_stats_mod[f'peaks_{driver}_perc'] = peaks_driver_n / peaks_all_n * 100\n",
    "        ds_peaks_stats_mod[f'peaks_{driver}_n'] = peaks_driver_n / ds_peaks_stats['ensemble'].size\n",
    "        # circular data\n",
    "#         circ = xs.doy_to_circ(ds_peaks_stats[f'cfi_{driver}_doy_mean'], dim=None)\n",
    "#         theta = xs.circ_stats.circ_mean(circ, dim='ensemble')\n",
    "#         ds_peaks_stats_mod[f'cfi_{driver}_doy_mean'] = xs.circ_to_doy(theta, None)\n",
    "#         ds_peaks_stats_mod[f'cfi_{driver}_doy_uniform_p'] = ds_peaks_stats[f'cfi_{driver}_doy_uniform_p'].max('ensemble')\n",
    "        \n",
    "else:\n",
    "    ds_peaks_stats_mod = ds_peaks_stats.drop(drop_var).sel(ensemble=model)\n",
    "\n",
    "df = ds_peaks_stats_mod.to_dataframe().rename(columns={'rivmth_lon':'lon', 'rivmth_lat':'lat'})\n",
    "df.to_csv(join(ddir, f'cfi_q{q}d{min_dist}_wdw{window_size}_{model}_rp{rp}_stats.csv'), float_format='%.4f')\n",
    "\n",
    "df = df[df['Qmean'] >= qmin]\n",
    "df['Qmean_log10'] = np.log10(df['Qmean'])\n",
    "df['markersize'] = np.maximum((df['Qmean_log10'] / df['Qmean_log10'].max()),0) * 40 + 2\n",
    "gdf = pandas2geopandas(df, crs={'init': 'epsg:4326'})\n",
    "N = len(df)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = {'savefig.bbox': 'tight',  'savefig.format': 'png', 'savefig.dpi':300}\n",
    "context = 'paper'\n",
    "# sns.set(context=context, style='whitegrid', font_scale=0.75 if context == 'talk' else 1., rc=rc)\n",
    "sns.set(context=context, style='whitegrid', font_scale=1.2 if context == 'paper' else 1., rc=rc)\n",
    "\n",
    "crs = ccrs.Robinson()\n",
    "crs_sub = ccrs.PlateCarree()\n",
    "\n",
    "cmap_div = sns.diverging_palette(220, 10, s=75, l=40, sep=1, as_cmap=True)\n",
    "\n",
    "bmap_kwargs = dict(\n",
    "    features=['land'],\n",
    "    feat_colors = [cfeature.COLORS['land_alt1']],\n",
    ")\n",
    "\n",
    "plot_kwargs=dict(edgecolor=(0.5, 0.5, 0.5, 0.8), linewidth=0.5, legend=False, zorder=2)\n",
    "box_kwargs=dict(whis=[5,95], showfliers=False, boxprops=dict(linewidth=1.5), medianprops=dict(linewidth=1.5))\n",
    "\n",
    "regions = {\n",
    "    'A': (-100.0, 22.0, -50.0, 53.0), # 30 x 18\n",
    "    'B': (-12.0, 30.0, 38.0, 60.0), # 50 x 30\n",
    "    'C': (100.0, 20.0, 150.0, 50.0),  # 50 x 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(colormap, x):\n",
    "    x = max(0.0, min(1.0, x))\n",
    "    a = int(x*255.0)\n",
    "    b = min(255, a + 1)\n",
    "    f = x*255.0 - a\n",
    "    return [colormap[a][0] + (colormap[b][0] - colormap[a][0]) * f,\n",
    "          colormap[a][1] + (colormap[b][1] - colormap[a][1]) * f,\n",
    "          colormap[a][2] + (colormap[b][2] - colormap[a][2]) * f]\n",
    "\n",
    "cticks_norm = np.array([0.1, 0.2, 0.5, 0.6, 0.7, 0.8, 0.9, 1.])\n",
    "cmap_data =[interpolate(google_turbo_data, x) for x in cticks_norm]\n",
    "cmap2 = ListedColormap(cmap_data)\n",
    "len(cmap_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map 1: peak percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['peaks_cmpnd_perc'].describe(percentiles=[0.1,0.25,0.75,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig = plt.figure(figsize=(17, 10))\n",
    "grid = plt.GridSpec(2, 6, hspace=0.1, wspace=0.4)\n",
    "\n",
    "column = 'peaks_cmpnd_perc'\n",
    "vmin, vmax= 0, 100 \n",
    "cticks=np.linspace(vmin, vmax, 11)\n",
    "gdf1 = gdf.copy().sort_values(column, ascending=True)\n",
    "\n",
    "\n",
    "# main map\n",
    "axg = fig.add_subplot(grid[:-1, :-1], projection=crs_sub)\n",
    "add_regions(axg, regions, dx=-6, dy=-6)\n",
    "basemap(axg, bbox=(-180, -60, 180, 90), gridlines=True, outline=True, **bmap_kwargs)\n",
    "plot_kwargs.update(markersize=gdf1['markersize'].values)\n",
    "cax = plot_choropleth(\n",
    "    fig, axg, gdf1, column=column, \n",
    "    cmap=cmap_turbo, vmin=vmin, vmax=vmax, cticks=cticks, discrete=True,\n",
    "    plot_kwargs=plot_kwargs,\n",
    "    cbar_kwargs=dict(label='percentage of water level peaks', location='right', extend='both'),\n",
    "    cbar_pos = dict(pad=0.1, fraction=0.1)\n",
    ")\n",
    "\n",
    "# submaps \n",
    "gdf1 = gdf1.sort_values('Qmean_log10', ascending=True)\n",
    "axs = dict()\n",
    "plot_kwargs.update(markersize=gdf1['markersize'].values*1.5)\n",
    "for i, (name, bbox) in enumerate(regions.items()):\n",
    "    ax = fig.add_subplot(grid[-1, i*2:(i+1)*2], projection=crs_sub)\n",
    "    basemap(ax, bbox=bbox, gridlines=True, gridspacing=10, outline=True, **bmap_kwargs)\n",
    "    plot_choropleth(\n",
    "        fig, ax, gdf1, column=column, \n",
    "        cmap=cmap_turbo, vmin=vmin, vmax=vmax, cticks=cticks, discrete=True,\n",
    "        plot_kwargs=plot_kwargs, cbar_kwargs=False,\n",
    "    )\n",
    "    xmin, ymax = bbox[0], bbox[3]\n",
    "    ax.text(xmin, ymax+1, name, transform=crs_sub)\n",
    "    axs[name] = ax\n",
    "    \n",
    "# boxplot ensemble\n",
    "box_kwargs.update(widths=0.5)\n",
    "color_props = dict(boxes=\"lightgrey\", whiskers=\"k\", medians=\"k\", caps=\"k\")\n",
    "ax = fig.add_subplot(grid[:-1, -1:])\n",
    "df_box = ds_peaks_stats[column].sel(index=gdf.index.values).to_series().unstack(-1).T * 100.\n",
    "df_box.plot.box(ax=ax, color=color_props, patch_artist=True, **box_kwargs)\n",
    "ax.set_ylim([cticks[1], cticks[-2]])\n",
    "ax.yaxis.set_ticks(cticks[1:-2])\n",
    "ax.yaxis.set_ticklabels([])\n",
    "ax.grid(axis='x')\n",
    "ax.set_xlabel('model')\n",
    "cpos = cax.get_position()\n",
    "bpos = ax.get_position()\n",
    "cax.set_position([cpos.x0, cpos.y0, cpos.width, cpos.height])\n",
    "# ax.set_position([cpos.x0+0.03, cpos.y0, bpos.width*1.5, cpos.height])\n",
    "ax.set_position([cpos.x0+0.03, cpos.y0+cpos.height*0.05, bpos.width*1.5, cpos.height/1.10])\n",
    "cax.yaxis.set_ticks_position('left')\n",
    "cax.yaxis.set_label_position('left')\n",
    "\n",
    "plt.savefig(join(fig_dir, f'peaks_q{q}d{min_dist}_wdw{window_size}_perc.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map 2: compound ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sign = np.logical_and(gdf['compound_ratio_sign']==1, gdf['compound_ratio_dir']==1)\n",
    "\n",
    "idx_sign.sum()/idx_sign.size, (gdf[idx_sign]['compound_ratio_mean']*100).describe(percentiles=[0.1,0.25,0.75,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig = plt.figure(figsize=(17, 10))\n",
    "grid = plt.GridSpec(2, 6, hspace=0.1, wspace=0.4)\n",
    "\n",
    "column = 'compound_ratio_mean'\n",
    "vmin, vmax= -0.10, .30\n",
    "cticks=np.linspace(vmin, vmax, 9)\n",
    "idx_sign = np.logical_and(gdf['compound_ratio_sign']==1, gdf['compound_ratio_dir']==1)\n",
    "idx_runoff = gdf[idx_sign]['main_driver']==-1\n",
    "idx_surge = gdf[idx_sign]['main_driver']==1\n",
    "gdf0 = gdf[idx_sign==False].copy()\n",
    "gdf1 = gdf[idx_sign].copy().sort_values(column, ascending=True)\n",
    "\n",
    "# main map\n",
    "axg = fig.add_subplot(grid[:-1, :-1], projection=crs_sub)\n",
    "add_regions(axg, regions, dx=-6, dy=-6)\n",
    "basemap(axg, bbox=(-180, -60, 180, 90), gridlines=True, outline=True, **bmap_kwargs)\n",
    "gdf0.plot(ax=axg, marker='x', color=(0, 0, 0, 0.8), markersize=5, linewidth=0.2, legend=False)\n",
    "\n",
    "plot_kwargs.update(markersize=gdf1[idx_surge]['markersize'].values, marker='^')\n",
    "plot_choropleth(\n",
    "    fig, axg, gdf1[idx_surge], column=column, \n",
    "    cmap=cmap2, vmin=vmin, vmax=vmax, cticks=cticks, discrete=True,\n",
    "    plot_kwargs=plot_kwargs, cbar_kwargs=False\n",
    ")\n",
    "plot_kwargs.update(markersize=gdf1[idx_runoff]['markersize'].values, marker='o')\n",
    "cax = plot_choropleth(\n",
    "    fig, axg, gdf1[idx_runoff], column=column, \n",
    "    cmap=cmap2, vmin=vmin, vmax=vmax, cticks=cticks, discrete=True,\n",
    "    plot_kwargs=plot_kwargs,\n",
    "    cbar_kwargs=dict(label=f'factor change return level (rp {rp} year) [-]', location='right', extend='both'),\n",
    "    cbar_pos = dict(pad=0.1, fraction=0.1)\n",
    ")\n",
    "\n",
    "# submaps \n",
    "axs = dict()\n",
    "gdf1 = gdf1.sort_values('Qmean_log10', ascending=True)\n",
    "for i, (name, bbox) in enumerate(regions.items()):\n",
    "    ax = fig.add_subplot(grid[-1, i*2:(i+1)*2], projection=crs_sub)\n",
    "    basemap(ax, bbox=bbox, gridlines=True, gridspacing=10, outline=True, **bmap_kwargs)\n",
    "    gdf0.plot(ax=ax, marker='x', color=(0, 0, 0, 0.8), markersize=15, linewidth=0.2, legend=False)\n",
    "    plot_kwargs.update(markersize=gdf1[idx_surge]['markersize'].values*2, marker='^')\n",
    "    plot_choropleth(\n",
    "        fig, ax, gdf1[idx_surge], column=column, \n",
    "        cmap=cmap2, vmin=vmin, vmax=vmax, cticks=cticks, discrete=True,\n",
    "        plot_kwargs=plot_kwargs, cbar_kwargs=False\n",
    "    )\n",
    "    plot_kwargs.update(markersize=gdf1[idx_runoff]['markersize'].values*2, marker='o')\n",
    "    plot_choropleth(\n",
    "        fig, ax, gdf1[idx_runoff], column=column, \n",
    "        cmap=cmap2, vmin=vmin, vmax=vmax, cticks=cticks, discrete=True,\n",
    "        plot_kwargs=plot_kwargs, cbar_kwargs=False\n",
    "    )\n",
    "    xmin, ymax = bbox[0], bbox[3]\n",
    "    ax.text(xmin, ymax+1, name, transform=crs_sub)\n",
    "    axs[name] = ax\n",
    "    \n",
    "# boxplot ensemble\n",
    "ax2 = fig.add_subplot(grid[:-1, -1:])\n",
    "# surge\n",
    "box_kwargs.update(widths=0.3)\n",
    "color_props = dict(boxes=\"lightgreen\", whiskers=\"k\", medians=\"k\", caps=\"k\")\n",
    "df_box = ds_peaks_stats[f'compound_ratio'].sel(rp=rp, index=gdf1[idx_surge].index).to_series().unstack(-1).T\n",
    "df_box.plot.box(ax=ax2, positions=np.arange(5)-0.2, color=color_props, patch_artist=True, **box_kwargs)\n",
    "# runoff\n",
    "color_props = dict(boxes=\"lightblue\", whiskers=\"k\", medians=\"k\", caps=\"k\")\n",
    "df_box = ds_peaks_stats[f'compound_ratio'].sel(rp=rp, index=gdf1[idx_runoff].index).to_series().unstack(-1).T\n",
    "df_box.plot.box(ax=ax2, positions=np.arange(5)+0.2, color=color_props, patch_artist=True, **box_kwargs)\n",
    "Qpatch = mpl.patches.Patch(color='lightblue', label='discharge')\n",
    "Hpatch = mpl.patches.Patch(color='lightgreen', label='surge')\n",
    "ax2.legend(handles=[Hpatch, Qpatch], ncol=2)\n",
    "\n",
    "ax2.set_ylim([cticks[1], cticks[-2]])\n",
    "ax2.yaxis.set_ticks(cticks[1:-2])\n",
    "ax2.yaxis.set_ticklabels([])\n",
    "ax2.grid(axis='x')\n",
    "ax2.set_xlabel('model')\n",
    "cpos = cax.get_position()\n",
    "bpos = ax2.get_position()\n",
    "cax.set_position([cpos.x0, cpos.y0, cpos.width, cpos.height])\n",
    "ax2.set_position([cpos.x0+0.03, cpos.y0+cpos.height*0.05, bpos.width*1.5, cpos.height/1.1])\n",
    "cax.yaxis.set_ticks_position('left')\n",
    "cax.yaxis.set_label_position('left')\n",
    "\n",
    "plt.savefig(join(fig_dir, f'peaks_q{q}d{min_dist}_wdw{window_size}_rp{rp:02d}_ratio.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 plot per location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xhydrostats as xs \n",
    "import scipy\n",
    "import pdb\n",
    "\n",
    "R = np.array([0.5, 1, 2, 5, 10, 15, 20, 25, 30, 34])\n",
    "R = np.arange(1,35)\n",
    "nyears = 35\n",
    "\n",
    "def _bootstrap_indexes(data, n_samples=1000):\n",
    "    return np.random.randint(data.shape[0], size=(n_samples, data.shape[0]))\n",
    "\n",
    "def _peaks2ev_single(peaks, peaks_n, peaks_lambda, R):\n",
    "    peaks_rank = scipy.stats.rankdata(-1. * peaks, method='ordinal')\n",
    "    peaks_prop = peaks_rank/float((peaks_n/peaks_lambda)-1) # weibull plotting position\n",
    "    return scipy.interpolate.interp1d(peaks_prop, peaks, bounds_error=False)(1./R)\n",
    "\n",
    "def _peaks2ev(peaks, nyears=nyears, R=R, ci=None, n_samples=10000):\n",
    "    peaks = peaks[np.isfinite(peaks)]\n",
    "    peaks_n = peaks.size\n",
    "    peaks_lambda = 1.\n",
    "    if nyears is not None:\n",
    "        peaks_lambda = peaks_n / nyears \n",
    "    if ci is not None:\n",
    "        peaks = peaks[_bootstrap_indexes(peaks, n_samples=n_samples)]\n",
    "        peaks_ev = np.percentile(\n",
    "            np.apply_along_axis(_peaks2ev_single, -1, peaks, peaks_n, peaks_lambda, R),\n",
    "            ci, axis=0\n",
    "        )\n",
    "    else:\n",
    "        peaks_ev = _peaks2ev_single(peaks, peaks_n, peaks_lambda, R)\n",
    "    return peaks_ev\n",
    "\n",
    "def _return_level(*args, **kwargs):\n",
    "    return np.apply_along_axis(_peaks2ev, -1, *args, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xhydrostats as xs \n",
    "import scipy\n",
    "import pdb\n",
    "\n",
    "def _peaks2ev_single(peaks, peaks_n, peaks_lambda, R):\n",
    "    peaks_rank = scipy.stats.rankdata(-1. * peaks, method='ordinal')\n",
    "    peaks_prop = peaks_rank/float((peaks_n/peaks_lambda)-1) # weibull plotting position\n",
    "    return scipy.interpolate.interp1d(peaks_prop, peaks, bounds_error=False)(1./R)\n",
    "\n",
    "def _peaks2ev(peaks, nyears, R, axis=None):\n",
    "    peaks = peaks[np.isfinite(peaks)]\n",
    "    peaks_n = peaks.size\n",
    "    peaks_lambda = 1.\n",
    "    if nyears is not None:\n",
    "        peaks_lambda = peaks_n / nyears \n",
    "#     print(peaks.size, peaks_n, peaks_lambda)\n",
    "    return _peaks2ev_single(peaks, peaks_n, peaks_lambda, R)\n",
    "\n",
    "def _return_level(arr, axis=-1, *args, **kwargs):\n",
    "    return np.apply_along_axis(_peaks2ev, arr=arr, axis=axis, *args, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ci(data, statfunction=np.nanmean, alpha=0.05, n_samples=10000, method='bca', statkwargs={}):\n",
    "    assert data.ndim==1, \"only tested for 1D arrays\"\n",
    "    data = data[np.isfinite(data)] # remove nans\n",
    "    n = data.size\n",
    "    assert n>0, \"empty array\"\n",
    "    alphas = np.array([alpha/2, 1-alpha/2])\n",
    "    stat = statfunction(data[np.random.randint(n, size=(n, n_samples))], axis=0, **statkwargs)\n",
    "    print(stat.shape)\n",
    "    assert stat.shape[-1] == n_samples\n",
    "    stat = np.apply_along_axis(np.sort, arr=stat, axis=0)\n",
    "    print(stat.shape)\n",
    "\n",
    "    if method == 'simple':\n",
    "        avals = alphas\n",
    "\n",
    "    elif method == 'bca':\n",
    "        if stat.ndim > 1:\n",
    "            raise NotImplementedError('bca method only implemented for statfunctions which return scalar values')\n",
    "        ostat = statfunction(data, axis=0, **statkwargs)\n",
    "        # The bias correction value.\n",
    "        z0 = norm.ppf(( 1.0*np.sum(stat < ostat, axis=0)) / n_samples )\n",
    "        # Statistics of the jackknife distribution\n",
    "        jstat = statfunction(data[_jackknife_sample(np.arange(n, dtype=np.int16))], axis=0, **statkwargs)\n",
    "        jmean = jstat.mean(axis=0)\n",
    "        # Acceleration value\n",
    "        av = np.sum((jmean - jstat)**3, axis=0) / (6.0 * np.sum((jmean - jstat)**2, axis=0)**1.5)\n",
    "        zs = z0 + norm.ppf(alphas).reshape(alphas.shape+(1,)*z0.ndim)\n",
    "        # Bias corrected alphas\n",
    "        avals = norm.cdf(z0 + zs/(1-av*zs))\n",
    "\n",
    "    #get confidence interval based on sorted bootstrapped statistic\n",
    "    nvals = np.nan_to_num(np.round((n_samples-1)*avals)).astype('int')\n",
    "\n",
    "    return stat[..., nvals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def confidence_interval(da, statfunction=np.nanmean, alpha=0.05, \n",
    "    n_samples=10000, method='bca', dim='time', statkwargs={}):\n",
    "    # confidence interval parameters\n",
    "    ci_kwargs = dict(\n",
    "        statfunction=statfunction, \n",
    "        alpha=alpha, \n",
    "        n_samples=n_samples, \n",
    "        method=method,\n",
    "        statkwargs=statkwargs\n",
    "        )\n",
    "    # apply ufunc parameters\n",
    "    output_sizes = {d:da[d].size for d in da.reset_coords(drop=True).coords if d != dim}\n",
    "    if 'R' in statkwargs:\n",
    "        output_sizes.update(rp= np.atleast_1d(statkwargs.get('R')).size)\n",
    "    kwargs = dict(               \n",
    "        input_core_dims=[[dim]], \n",
    "        output_core_dims=[['ci_bounds', 'rp']],\n",
    "        dask='allowed', \n",
    "        output_dtypes=[np.float],       \n",
    "        # on output, <dim> is reduced to length 2 with upper and lower ci bounds\n",
    "        output_sizes={'ci_bounds': 2, 'rp': 2},\n",
    "        # vectorize to apply over 1 dim\n",
    "        vectorize=True\n",
    "    )\n",
    "    print(kwargs)\n",
    "    # apply ci_nd over dim\n",
    "    ci = xr.apply_ufunc(_ci, da, kwargs=ci_kwargs, **kwargs)\n",
    "    ci['ci_bounds'] = xr.Variable('ci_bounds', np.array([alpha/2, 1-alpha/2]))\n",
    "    return ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_cmf['water_level'].isel(index=slice(2), ensemble=slice(2)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_interval(ds_test.chunk({'time':-1}), statfunction=_return_level, n_samples=100, method='simple', \n",
    "                    statkwargs=dict(R=np.array([2., 10.]), nyears=35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[4, 2], \n",
    "              [3, 5],\n",
    "              [2, 1]])\n",
    "np.apply_along_axis(np.sort, arr=a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyears= 35\n",
    "loc = 1617\n",
    "model='cnrs'\n",
    "rm = {'cmpnd': 'compound', 'runoff': 'discharge'}\n",
    "rm_vars = {'surge_wdw_max': 'y', 'runoff_wdw_max': 'x', 'peaks_all': 'i'}\n",
    "ds_loc = xr.merge([ds_peaks.rename(rm_vars)[['x', 'y', 'i']], ds_cmf['water_level']]).sel(index=loc, ensemble=model)\n",
    "ds_loc.scen.data = [rm.get(n, n) for n in ds_loc.scen.data]\n",
    "ds_loc = ds_loc.where(ds_loc['i'])\n",
    "h = ds_loc['water_level'].isel(scen=0).values #.dropna('time', how = 'all').values\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_return_level(h, nyears=35, R=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.bootstrap._ci(h, statfunction=_return_level, n_samples=10000, statkwargs=dict(R=10, nyears=35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def _peaks2rp_single(peaks, nyears):\n",
    "    valid = np.isfinite(peaks)\n",
    "    rp = np.ones(peaks.size)*np.nan\n",
    "    peaks = peaks[valid]\n",
    "    peaks_n = valid.size\n",
    "    peaks_lambda = 1.\n",
    "    if nyears is not None:\n",
    "        peaks_lambda = peaks_n / nyears \n",
    "    peaks_rank = scipy.stats.rankdata(-1. * peaks, method='ordinal')\n",
    "    peaks_prop = peaks_rank/float((peaks_n/peaks_lambda)-1) # weibull plotting position\n",
    "    rp[valid] = 1/peaks_prop\n",
    "    return rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xs.confidence_interval(ds_loc['water_level'], statfunction=_peaks2rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R = np.array([1.1, 1.2, 1.5, 2, 3, 4, 5, 10, 15, 20, 25, 30, 34])\n",
    "# nyears = 35\n",
    "# def _peaks2ev_single(peaks, peaks_n, peaks_lambda, R):\n",
    "#     peaks_rank = scipy.stats.rankdata(-1. * peaks, method='ordinal')\n",
    "#     peaks_prop = peaks_rank/float((peaks_n/peaks_lambda)-1) # weibull plotting position\n",
    "#     return scipy.interpolate.interp1d(peaks_prop, peaks, bounds_error=False)(1./R)\n",
    "\n",
    "# def _peaks2ev(peaks, nyears, R, ci=[5, 50, 95], n_samples=10000):\n",
    "#     peaks = peaks[np.isfinite(peaks)]\n",
    "#     peaks_n = peaks.size\n",
    "#     peaks_lambda = 1.\n",
    "#     if nyears is not None:\n",
    "#         peaks_lambda = peaks_n / nyears \n",
    "#     if ci is not None:\n",
    "#         peaks = peaks[_bootstrap_indexes(peaks, n_samples=n_samples)]\n",
    "#         peaks_ev = np.percentile(\n",
    "#             np.apply_along_axis(_peaks2ev_single, -1, peaks, peaks_n, peaks_lambda, R),\n",
    "#             ci, axis=0\n",
    "#         )\n",
    "#     else:\n",
    "#         peaks_ev = _peaks2ev_single(peaks, peaks_n, peaks_lambda, R)\n",
    "#     return peaks_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "# [1617, 1230, 483, 1646]\n",
    "nyears= 35\n",
    "loc = 1617\n",
    "model='cnrs'\n",
    "rm = {'cmpnd': 'compound', 'runoff': 'discharge'}\n",
    "scen_cmap={'compound': 'darkorange', 'discharge': 'green', 'surge': 'blue'}\n",
    "scen_mmap={'compound': '^', 'discharge': 'o', 'surge': 'x'}\n",
    "ls={'compound': '-', 'discharge': '--', 'surge': '-.'}\n",
    "scenarios = ['surge', 'discharge', 'compound']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "\n",
    "rm_vars = {'surge_wdw_max': 'y', 'runoff_wdw_max': 'x', 'peaks_all': 'i'}\n",
    "ds_loc = xr.merge([ds_peaks.rename(rm_vars)[['x', 'y', 'i']], ds_cmf['water_level']]).sel(index=loc, ensemble=model)\n",
    "ds_loc.scen.data = [rm.get(n, n) for n in ds_loc.scen.data]\n",
    "ds_loc = ds_loc.where(ds_loc['i'])\n",
    "for scen in ds_loc.scen.data:\n",
    "    ds_loc[f'rp_{scen}'] = xr.Variable('time', _peaks2rp_single(ds_loc['water_level'].sel(scen=scen).values, nyears))\n",
    "    \n",
    "# get compound water levels and drivers\n",
    "scen = 'compound'\n",
    "ds_loc_scen = ds_loc[['x', 'y', 'i', f'rp_{scen}']].sel(scen=scen).rename({f'rp_{scen}': 'rp'})\n",
    "del_vars = [v for v in ds_loc_scen.coords if v != 'time']\n",
    "ds_loc_scen = ds_loc_scen.where(ds_loc_scen['i']==1, drop=True).drop(del_vars)\n",
    "df = ds_loc_scen.to_dataframe()\n",
    "\n",
    "\n",
    "rps = [0.2, 1, 2, 5, 10, 20, 30]\n",
    "cmap = ListedColormap(google_turbo_data[50:])\n",
    "cmap.set_over('black')\n",
    "norm = BoundaryNorm(rps, cmap.N)\n",
    "\n",
    "fig, (ax, ax2) = plt.subplots(1, 2, figsize=(17,5))\n",
    "\n",
    "df.plot.scatter(ax=ax, y='y', x='x', c='rp', s=df['rp'].values+25, cmap=cmap, norm=norm, colorbar=False)\n",
    "cax, _ = mpl.colorbar.make_axes_gridspec(ax, orientation='vertical', fraction=0.2)\n",
    "cbar = mpl.colorbar.ColorbarBase(cax, cmap=cmap, norm=norm, extend='max')\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ax.hlines(0, *xlim, linewidth=0.5, linestyle='--')\n",
    "ax.vlines(0, *ylim, linewidth=0.5, linestyle='--')\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlabel('discharge anomaly [m$^3$/s]')\n",
    "ax.set_ylabel('surge [m]')\n",
    "cax.set_ylabel('return period [years]')\n",
    "# ax.set_title(f'compound POT (threshold = {q}th percentile)')\n",
    "\n",
    "# rps = [2, 5, 10, 20, 35]\n",
    "for scen in ds_loc.scen.data:\n",
    "    ds_loc_scen = ds_loc[['water_level', f'rp_{scen}', 'i']].sel(scen=scen).rename({f'rp_{scen}': 'rp'})\n",
    "    ds_loc_scen = ds_loc_scen.where(ds_loc_scen['i']==1, drop=True).drop(del_vars)\n",
    "    x = ds_loc_scen['rp'].values\n",
    "    y = ds_loc_scen['water_level'].values\n",
    "#     ax2.plot(R, _return_level(y), c=scen_cmap[scen])\n",
    "    ci = _return_level(y, ci=[5, 95])\n",
    "    kwargs = dict(linewidth=1, color=scen_cmap[scen], linestyle=':', zorder=1)\n",
    "    for i in range(ci.shape[0]):\n",
    "        ax2.plot(R, ci[i,:], **kwargs)\n",
    "    ax2.scatter(x=x, y=y, label=scen, c=scen_cmap[scen], marker=scen_mmap[scen], zorder=2)\n",
    "ax2.legend()\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlim([0.25, 40])\n",
    "ax2.set_xticks(rps)\n",
    "ax2.set_xticklabels(rps)\n",
    "ax2.set_xlabel('return period [years]')\n",
    "ax2.set_ylabel('water level [m +EGM96]')\n",
    "\n",
    "# plt.savefig(join(fig_dir, f'peaks_q{q}d{min_dist}_wdw{window_size}_loc{loc}_{model}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "tslice = slice('12-01-2002', '06-01-2003')\n",
    "ds_loc = ds_cmf.sel(index=loc, ensemble=model, time=tslice)\n",
    "ds_loc.scen.data = [rm.get(n, n) for n in ds_loc.scen.data]\n",
    "\n",
    "# plot timeseries \n",
    "for d in scenarios:\n",
    "    i = 0\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(17, 5), sharex=True)\n",
    "    ds_loc['sea_water_level'].plot.line(x='time', ax=axs[0], label='tide + seas. SL + surge', \n",
    "        c='blue' if d!='discharge' else 'grey', zorder = 2 if d!='discharge' else 1)\n",
    "    ds_loc['sea_water_level_climatology'].plot.line(x='time', ax=axs[0], label='tide + seas. SL', \n",
    "        c='blue' if d=='discharge' else 'grey', zorder = 2 if d=='discharge' else 1, linestyle='--')\n",
    "    axs[0].set_ylabel('water level [m+EGM96]')\n",
    "    axs[0].legend(ncol=2, loc='lower center')\n",
    "    \n",
    "    for dd in scenarios:\n",
    "        ds_loc['water_level'].sel(scen=dd).plot.line(x='time', ax=axs[1], label=dd, add_legend=False,\n",
    "            linestyle=ls[dd], c=scen_cmap[dd] if dd==d else 'lightgrey', zorder = 2 if dd==d else 1)\n",
    "    axs[1].set_ylabel('water level [m+EGM96]')\n",
    "    axs[1].legend(ncol=3, loc='lower center')\n",
    "\n",
    "    ds_loc['river_discharge'].plot.line(x='time', ax=axs[2], label='discharge',\n",
    "            c='green' if d != 'surge' else 'grey', zorder = 2 if d != 'surge' else 1)\n",
    "    ds_loc['river_discharge_climatology'].plot.line(x='time', ax=axs[2], linestyle='--', label='discharge climatology',\n",
    "            c='green' if d == 'surge' else 'grey', zorder = 2 if d == 'surge' else 1, )\n",
    "    axs[2].set_ylabel('discharge [m$^3$/s]')\n",
    "    axs[2].legend(ncol=2, loc='lower center')\n",
    "\n",
    "    axs[0].set_title('downstream')\n",
    "    axs[1].set_title('river mouth')\n",
    "    axs[2].set_title('upstream')\n",
    "    \n",
    "    fig.suptitle(d, size=14, weight='bold')\n",
    "    \n",
    "    plt.savefig(join(fig_dir, f'ts_{model}_loc{loc}_{d}.png'))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
