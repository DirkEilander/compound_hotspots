{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from copy import deepcopy\n",
    "# suppress some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load one local library with additional statistics for the xarray datastructure\n",
    "import xhydrostats as xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = r'/scratch/compound'\n",
    "t0, t1 = datetime(1980,1,1), datetime(2014,12,30)\n",
    "q = 95\n",
    "min_dist = 45\n",
    "window_size = 7\n",
    "Npeaks = 50\n",
    "chunks = {'ensemble':-1, 'time':-1, 'index':250}\n",
    "drivers = ['compound', 'surge', 'runoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_attrs = dict(\n",
    "    title = 'statistics of global compound river mouth reanalysis data',\n",
    "    source = 'global compound river mouth reanalysis and global tide and surge reanalysis',\n",
    "    institution = 'Institute for Environmental Studies (IVM) - Vrije Universiteit Amsterdam',\n",
    "    author = 'Dirk Eilander (dirk.eilander@vu.nl)',\n",
    "    conventions = \"CF-1.7\",\n",
    "    date_created = str(datetime.now().date()),\n",
    "    history = 'created using xarray v' + xr.__version__,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load regular outputs\n",
    "fn_gcfr = join(root, 'gcfr.zarr')\n",
    "fn_out_drivers = join(root, 'cfi_drivers.nc')\n",
    "ds_cmf = xr.open_zarr(fn_gcfr).sel(time=slice(t0,t1))\n",
    "ds_cmf.ensemble.data = ds_cmf.ensemble.values.astype(str)\n",
    "\n",
    "# TODO: merge instant sealvl instead of daily max. fix downstream\n",
    "ds_sealvl = xr.open_dataset(join(root, 'sealvl_inst.nc')).drop('gtsm_idx').sel(time=slice(t0,t1))\n",
    "ds_sealvl['sea_surge'] = ds_sealvl['sea_water_level'] - ds_sealvl['sea_water_level_climatology'] \n",
    "ds_cmf = xr.merge([ds_sealvl, ds_cmf.drop(['sea_water_level', 'sea_water_level_climatology'])])\n",
    "ds_cmf['river_discharge_anomaly'] = ds_cmf['river_discharge'] - ds_cmf['river_discharge_climatology'] \n",
    "\n",
    "# remove untrusted months -> something funny in gtsm during oct/nov 1990\n",
    "ds_cmf = ds_cmf.where(ds_cmf.time!=slice(datetime(1990,10,1), datetime(1991,3,1))) #.sel(index=[1617, 1230])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_peaks = join(root, f'peaks_q{q}d{min_dist}_wdw{window_size}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_wl = ds_cmf['water_level'].chunk(chunks)\n",
    "# peaks (boolean) per scenario\n",
    "peaks = xr.ufuncs.isfinite(xs.get_peaks(da_wl, min_dist=min_dist, dim='time')).reindex_like(da_wl, fill_value=False).chunk(chunks)\n",
    "peaks.name = 'peaks_all'\n",
    "# max water level within +/- 3 days for peaks per scenario\n",
    "wl_wdwmax = da_wl.rolling(time=window_size, min_periods=1, center=True).construct('window').max('window')\n",
    "# keep only largest peaks within +/- 3 days between all scen\n",
    "peaks_max = xr.where(peaks, da_wl>=wl_wdwmax.max('scen'), False).chunk(chunks)\n",
    "# remove peaks from compound scen where as large as in single scen\n",
    "peaks_max_n = peaks_max.sum('scen')\n",
    "peaks_lst = []\n",
    "for driver in peaks_max.scen.data:\n",
    "    if driver == 'cmpnd':\n",
    "        # only compound if compound scenario gives SINGLE largest water level\n",
    "        peaks_driver = xr.where(np.logical_and(peaks_max_n==1, peaks_max.sel(scen=driver)), True, False)\n",
    "    else:\n",
    "        peaks_driver = xr.where(np.logical_and(peaks_max_n>=1, peaks_max.sel(scen=driver)), True, False)\n",
    "    peaks_lst.append(peaks_driver)\n",
    "peaks_max = xr.concat(peaks_lst, dim='scen')\n",
    "# peaks_max['scen'] = peaks_max.scen\n",
    "peaks_max_n = peaks_max.sum('scen')\n",
    "assert peaks_max_n.max() == 1\n",
    "\n",
    "# rank peaks\n",
    "peaks_val_flat = xr.where(peaks_max, da_wl, -np.inf).max('scen').chunk(chunks)\n",
    "peaks_ranks_flat = xs.rankdata(-1 * peaks_val_flat, dim='time', method='ordinal') # minus peak values to sort from large to small\n",
    "peaks_rank = xr.where(peaks_max, peaks_ranks_flat, np.nan)\n",
    "peaks_rank.name = 'peaks_rank'\n",
    "\n",
    "# add drivers \n",
    "surge_wdw_max = ds_cmf['sea_surge'].rolling(time=window_size, min_periods=1, center=True).construct('window').max('window')\n",
    "surge_wdw_max.name = 'surge_wdw_max'\n",
    "runoff_wdw_max = ds_cmf['river_discharge_anomaly'].rolling(time=window_size, min_periods=1, center=True).construct('window').max('window')\n",
    "runoff_wdw_max.name = 'runoff_wdw_max'\n",
    "\n",
    "ds_peaks = xr.merge([peaks, peaks_rank, surge_wdw_max, runoff_wdw_max], join='inner').chunk(chunks)\n",
    "ds_peaks.to_netcdf(fn_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### peak percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce time index\n",
    "ds_peaks = xr.open_dataset(fn_peaks, chunks=chunks) #.sel(index=[1617, 1230])\n",
    "\n",
    "# select top Npeaks\n",
    "peaks_top = ds_peaks[f'peaks_rank'].where(ds_peaks[f'peaks_rank']<=Npeaks)\n",
    "peaks_top_n = peaks_top.sum('time')\n",
    "peaks_top_n.name = f'peaks_n'\n",
    "# calculate statistics for top Npeaks\n",
    "cmpnd_stats = peaks_top_n.to_dataset()\n",
    "cmpnd_stats[f'peaks_perc'] =  peaks_top_n / peaks_top_n.sum('scen')\n",
    "cmpnd_stats[f'peaks_Hmean'] = ds_peaks['surge_wdw_max'].where(peaks_top).mean('time')\n",
    "cmpnd_stats[f'peaks_Qmean'] = ds_peaks['runoff_wdw_max'].where(peaks_top).mean('time')\n",
    "# calculate overal statistics\n",
    "cmpnd_stats[f'Qmean'] = ds_cmf['river_discharge'].mean(['time'])\n",
    "cmpnd_stats[f'Qmamax'] = ds_cmf['river_discharge'].resample(time = 'A').max('time').mean(['time'])\n",
    "cmpnd_stats[f'Hmamax'] = ds_cmf['sea_surge'].resample(time = 'A').max('time').mean(['time'])\n",
    "# close input and save to disk\n",
    "ds_peaks.close()\n",
    "fn_out = join(root, f'peaks{Npeaks}_q{q}d{min_dist}_wdw{window_size}_perc.nc')\n",
    "cmpnd_stats.attrs.update(**glob_attrs)\n",
    "cmpnd_stats = cmpnd_stats.chunk({k:v for k,v in chunks.items() if k != 'time'})\n",
    "encoding = {k: {'zlib': True} for k in cmpnd_stats.data_vars}\n",
    "cmpnd_stats.to_netcdf(fn_out, mode='w', encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### return levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce time index\n",
    "rp = np.array([1, 2, 5, 10, 20, 30, 34])\n",
    "\n",
    "ds_peaks = xr.open_dataset(fn_peaks, chunks=chunks) #.sel(index=[1617, 1230])\n",
    "da_peaks = ds_cmf['water_level'].chunk(chunks).where(ds_peaks['peaks_all'])\n",
    "wl_extr = xs.return_level(da_peaks, nyears=35, rp=rp)\n",
    "wl_extr.name = 'return_level'\n",
    "wl_extr_ci = xs.return_level_ci(da_peaks, nyears=35, rp=rp, alphas=np.array([0.1, 0.25, 0.75, 0.9]), n_samples=1000)\n",
    "N = ds_peaks['ensemble'].size\n",
    "\n",
    "ds = wl_extr.to_dataset()\n",
    "ds['return_level_ci'] = wl_extr_ci\n",
    "ds['main_driver'] = xr.where(\n",
    "    wl_extr.sel(scen='runoff').mean('ensemble') > wl_extr.sel(scen='surge').mean('ensemble'), \n",
    "    1, # Q \n",
    "    -1 # H\n",
    ") \n",
    "wl_extr_single = xr.where(ds['main_driver']==1, wl_extr.sel(scen='runoff'), wl_extr.sel(scen='surge'))\n",
    "wl_extr_ci_single = xr.where(ds['main_driver']==1, wl_extr_ci.sel(scen='runoff'), wl_extr_ci.sel(scen='surge'))\n",
    "ds['compound_ratio'] = (wl_extr.sel(scen='cmpnd') - wl_extr_single) / wl_extr_single\n",
    "ds['compound_ratio_mean'] = ds['compound_ratio'].mean('ensemble')\n",
    "compound_ratio_std = ds['compound_ratio'].std('ensemble')\n",
    "compound_ratio_cv = ds['compound_ratio_mean'] / compound_ratio_std\n",
    "ds['compound_ratio_dir'] =  xr.ufuncs.fabs(xr.ufuncs.sign(ds['compound_ratio']).sum('ensemble')) == N\n",
    "ds['compound_ratio_sign0'] = xr.ufuncs.fabs(compound_ratio_cv) > (2 / xr.ufuncs.sqrt(N-1))\n",
    "\n",
    "ds['compound_ratio_sign1'] = xr.where(\n",
    "    ds['compound_ratio']>0,\n",
    "    wl_extr_ci.sel(scen='cmpnd', alpha=0.1) >= wl_extr_ci_single.sel(alpha=0.9),\n",
    "    wl_extr_ci_single.sel(alpha=0.1) >= wl_extr_ci.sel(scen='cmpnd', alpha=0.9),\n",
    ").sum('ensemble')\n",
    "\n",
    "# save output to nc\n",
    "fn_out = join(root, f'peaks_q{q}d{min_dist}_wdw{window_size}_rp.nc')\n",
    "ds.attrs.update(**glob_attrs)\n",
    "ds = ds.chunk({k:v for k,v in chunks.items() if k != 'time'})\n",
    "encoding = {k: {'zlib': True} for k in ds.data_vars}\n",
    "ds.to_netcdf(fn_out, mode='w', encoding=encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output to nc\n",
    "# ds_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# components\n",
    "da_cmpnd = ds_cmf['water_level'].sel(scen='cmpnd').drop('scen').chunk(chunks)\n",
    "da_surge = ds_cmf['water_level'].sel(scen='surge').drop('scen').chunk(chunks)\n",
    "da_runoff = ds_cmf['water_level'].sel(scen='runoff').drop('scen').chunk(chunks)\n",
    "\n",
    "# compute cfi\n",
    "da_surge_wdwmax = da_surge.rolling(time=window_size, min_periods=1, center=True).max().chunk(chunks)\n",
    "da_runoff_wdwmax = da_runoff.rolling(time=window_size, min_periods=1, center=True).max().chunk(chunks)\n",
    "da_cmpnd_std = da_cmpnd.std('time')\n",
    "cfi_surge = ((da_cmpnd - da_runoff_wdwmax) / da_cmpnd_std).chunk(chunks)\n",
    "cfi_surge.name = 'cfi_surge'\n",
    "cfi_runoff = ((da_cmpnd - da_surge_wdwmax) / da_cmpnd_std).chunk(chunks)\n",
    "cfi_runoff.name = 'cfi_runoff'\n",
    "cfi_compound = xr.where(xr.ufuncs.fabs(cfi_runoff)<xr.ufuncs.fabs(cfi_surge), cfi_runoff, cfi_surge)\n",
    "cfi_compound.name = 'cfi_compound'\n",
    "\n",
    "# compound water level peaks\n",
    "threshold = xs.nanpercentile(da_cmpnd, q=q, dim='time')\n",
    "peaks = xr.ufuncs.isfinite(xs.peaks_over_threshold(da_cmpnd, min_dist=min_dist, threshold=threshold, dim='time', chunks=chunks))\n",
    "peaks = peaks.reindex_like(cfi_compound, fill_value=False).chunk(chunks)\n",
    "peaks.name = 'peaks_all'\n",
    "# classify peaks\n",
    "peaks_compound = xr.where(peaks, np.logical_or(np.logical_and(cfi_surge>0,  cfi_runoff>0),\n",
    "                                               np.logical_and(cfi_surge<0,  cfi_runoff<0)), False)\n",
    "peaks_compound.name = 'peaks_compound'\n",
    "peaks_surge    = xr.where(peaks, np.logical_and(cfi_surge>=0,  cfi_runoff<=0), False)\n",
    "peaks_surge.name = 'peaks_surge'\n",
    "peaks_runoff   = xr.where(peaks, np.logical_and(cfi_surge<=0, cfi_runoff>=0), False)\n",
    "peaks_runoff.name = 'peaks_runoff'\n",
    "\n",
    "# add drivers \n",
    "surge_driver = ds_cmf['sea_surge'].rolling(time=window_size, min_periods=1, center=True).max()\n",
    "surge_driver.name = 'surge_wdw_max'\n",
    "runoff_driver = ds_cmf['river_discharge'].rolling(time=window_size, min_periods=1, center=True).max()\n",
    "runoff_driver.name = 'runoff_wdw_max'\n",
    "\n",
    "ds_cfi = xr.merge([\n",
    "    cfi_runoff, cfi_surge, cfi_compound,\n",
    "    surge_driver, runoff_driver,\n",
    "    peaks_runoff, peaks_surge, peaks_compound, peaks,\n",
    "], join='inner').chunk(chunks)\n",
    "ds_cfi.to_netcdf(join(root, f'cfi_q{q}d{min_dist}_wdw{window_size}.nc'))\n",
    "# ds_cfi.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn_out = join(root, f'cfi_q{q}d{min_dist}_wdw{window_size}_stats.nc')\n",
    "fn_cfi = join(root, f'cfi_q{q}d{min_dist}_wdw{window_size}.nc')\n",
    "\n",
    "ds_cfi = xr.open_dataset(fn_cfi, chunks=chunks)\n",
    "drivers = ['compound', 'surge', 'runoff']\n",
    "\n",
    "peaks_all_n = ds_cfi['peaks_all'].sum('time')\n",
    "peaks_all_n.name = 'peaks_all_n'\n",
    "cmpnd_stats = peaks_all_n.to_dataset()\n",
    "for driver in drivers:\n",
    "    peaks_driver = ds_cfi[f'peaks_{driver}']\n",
    "    cfi_driver = ds_cfi[f'cfi_{driver}']\n",
    "    cfi_driver_peaks = cfi_driver.where(peaks_driver)\n",
    "    # percentage of peaks\n",
    "    peaks_driver_n = peaks_driver.sum('time') \n",
    "    cmpnd_stats[f'peaks_{driver}_n'] = peaks_driver_n\n",
    "    cmpnd_stats[f'peaks_{driver}_perc'] = peaks_driver_n / peaks_all_n\n",
    "    # cfi > 1 std \n",
    "    cmpnd_stats[f'peaks_{driver}_n_cfi1'] = xr.where(cfi_driver_peaks>1, 1, 0).sum('time')\n",
    "    cmpnd_stats[f'peaks_{driver}_n_cfi2'] = xr.where(cfi_driver_peaks>2, 1, 0).sum('time')\n",
    "    # cfi stats\n",
    "    cmpnd_stats[f'cfi_{driver}'] = cfi_driver_peaks.mean(dim='time')\n",
    "    cmpnd_stats[f'cfi_{driver}_std'] = cfi_driver_peaks.std(dim='time')\n",
    "    # CV with abs mean\n",
    "    cmpnd_stats[f'cfi_{driver}_cv'] = xr.ufuncs.fabs(cmpnd_stats[f'cfi_{driver}']) / cmpnd_stats[f'cfi_{driver}_std']\n",
    "    # CV > 2 / sqrt(N-1)\n",
    "    cmpnd_stats[f'cfi_{driver}_sign'] = (cmpnd_stats[f'cfi_{driver}_cv'] > (2 / xr.ufuncs.sqrt(peaks_driver_n-1))) * np.sign(cmpnd_stats[f'cfi_{driver}'])\n",
    "    \n",
    "cmpnd_stats[f'river_discharge_mean'] = ds_cmf['river_discharge'].mean(['time'])\n",
    "cmpnd_stats[f'river_discharge_mean_amax'] = ds_cmf['river_discharge'].resample(time = 'A').max('time').mean(['time'])\n",
    "cmpnd_stats[f'sea_surge_mean_amax'] = ds_cmf['sea_surge'].resample(time = 'A').max('time').mean(['time'])\n",
    "\n",
    "peaks_test_n = (cmpnd_stats['peaks_compound_n'] + cmpnd_stats['peaks_runoff_n'] + cmpnd_stats['peaks_surge_n'])\n",
    "assert np.all(cmpnd_stats['peaks_all_n'] == peaks_test_n ).compute().values\n",
    "\n",
    "datasets = [cmpnd_stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for driver in drivers:       \n",
    "    cfi_peaks = ds_cfi[f'cfi_{driver}'].where(ds_cfi[f'peaks_{driver}'])\n",
    "    # timing of cfi\n",
    "    cmpnd_temp_stats = xs.mean_flood_day_stats(cfi_peaks.fillna(-np.inf))\n",
    "    cmpnd_temp_stats = cmpnd_temp_stats.rename({k:f'cfi_{driver}_{k}' for k in cmpnd_temp_stats.data_vars})\n",
    "    datasets.append(cmpnd_temp_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save output to nc\n",
    "\n",
    "ds_out = xr.merge(datasets)\n",
    "ds_out.attrs = dict(\n",
    "    title = 'statistics of global compound river mouth reanalysis data',\n",
    "    source = 'global compound river mouth reanalysis and global tide and surge reanalysis',\n",
    "    institution = 'Institute for Environmental Studies (IVM) - Vrije Universiteit Amsterdam',\n",
    "    author = 'Dirk Eilander (dirk.eilander@vu.nl)',\n",
    "    conventions = \"CF-1.7\",\n",
    "    date_created = str(datetime.now().date()),\n",
    "    history = 'created using xarray v' + xr.__version__,\n",
    ")\n",
    "\n",
    "ds_out = ds_out.chunk({k:v for k,v in chunks.items() if k != 'time'})\n",
    "encoding = {k: {'zlib': True} for k in ds_out.data_vars}\n",
    "ds_out.to_netcdf(fn_out, mode='w', encoding=encoding)\n",
    "# ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce index/space dimension\n",
    "\n",
    "fn_out = join(root, f'cfi_q{q}d{min_dist}_wdw{window_size}_stats2.csv')\n",
    "fn_cfi_stats = join(root, f'cfi_q{q}d{min_dist}_wdw{window_size}_stats.nc')\n",
    "\n",
    "cmpnd_stats = xr.open_dataset(fn_cfi_stats, chunks={'index': -1, 'ensemble':-1}).drop('percentile')\n",
    "drivers = ['compound', 'surge', 'runoff']\n",
    "\n",
    "peaks_all_n = cmpnd_stats['peaks_all_n'].sum('index')\n",
    "peaks_all_n.name = 'peaks_all_n'\n",
    "cmpnd_stats2 = peaks_all_n.to_dataset()\n",
    "for driver in drivers:\n",
    "    cfi_driver = cmpnd_stats[f'cfi_{driver}']\n",
    "    cmpnd_stats2[f'peaks_{driver}_perc'] = cmpnd_stats[f'peaks_{driver}_n'].sum('index') / peaks_all_n\n",
    "    cmpnd_stats2[f'peaks_{driver}_perc_cfi1'] = cmpnd_stats[f'peaks_{driver}_n_cfi1'].sum('index') / peaks_all_n\n",
    "    cmpnd_stats2[f'peaks_{driver}_perc_cfi2'] = cmpnd_stats[f'peaks_{driver}_n_cfi2'].sum('index') / peaks_all_n\n",
    "    cmpnd_stats2[f'cfi_{driver}_min'] = cfi_driver.min('index') \n",
    "    cmpnd_stats2[f'cfi_{driver}_max'] = cfi_driver.max('index')\n",
    "    cfi_q = xs.nanpercentile(cfi_driver, [5, 50 ,95], dim='index')\n",
    "    cmpnd_stats2[f'cfi_{driver}_med'] = cfi_q.sel(percentile=50).drop('percentile')\n",
    "    cmpnd_stats2[f'cfi_{driver}_p05'] = cfi_q.sel(percentile=5).drop('percentile')\n",
    "    cmpnd_stats2[f'cfi_{driver}_p95'] = cfi_q.sel(percentile=95).drop('percentile')\n",
    "\n",
    "\n",
    "cmpnd_stats2.to_dataframe().T.to_csv(fn_out, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,6))\n",
    "\n",
    "sel = dict(time=slice('01-01-1995', '09-01-1996'))\n",
    "sel = dict(time=slice('01-01-1991', '09-01-1991'))\n",
    "da_wl.isel(index=0, ensemble=1).sel(**sel).plot.line(x='time', ax=ax)\n",
    "xr.where(np.logical_and(peaks100, peaks.sum('scen')==2), peaks_val, np.nan).isel(index=0, ensemble=1).sel(**sel).to_series().plot(ax=ax, linewidth=0, markersize=10, marker='x')\n",
    "\n",
    "fig2, ax2 = plt.subplots(1,1,figsize=(10,6))\n",
    "# ds_cmf['river_discharge'].isel(index=0, ensemble=1).sel(**sel).plot.line(x='time', ax=ax2)\n",
    "ds_cmf['sea_surge'].isel(index=0).sel(**sel).plot.line(x='time', ax=ax2)\n",
    "# ds_cmf['sea_water_level_climatology'].isel(index=0).sel(**sel).plot.line(x='time', ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "driver = 'compound'\n",
    "cfi_driver = cmpnd_stats[f'peaks_{driver}_perc']*100\n",
    "cfi_driver.to_series().unstack(-1).T.plot.box(whis=[0,95], showfliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = 'compound'\n",
    "cfi_driver = cmpnd_stats[f'cfi_{driver}']\n",
    "cfi_driver.to_series().unstack(-1).T.plot.box(whis=[0,95], showfliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test for single station\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "q = 95\n",
    "min_dist = 30\n",
    "rid = 387\n",
    "# rid = 3200\n",
    "\n",
    "\n",
    "ds_i= ds_cmf.isel(index=rid).sel(ensemble='nerc').squeeze().load()\n",
    "\n",
    "# components\n",
    "da_cmpnd = ds_i['water_level'].sel(scen='cmpnd') \n",
    "da_surge = ds_i['water_level'].sel(scen='surge')\n",
    "da_runoff = ds_i['water_level'].sel(scen='runoff')\n",
    "da_surge_wdwmax = da_surge.rolling(time=7, min_periods=1, center=True).max()\n",
    "da_runoff_wdwmax = da_runoff.rolling(time=7, min_periods=1, center=True).max()\n",
    "da_cmpnd_std = da_cmpnd.std('time')\n",
    "cfi_surge = ((da_cmpnd - da_runoff_wdwmax) / da_cmpnd_std)\n",
    "cfi_runoff = ((da_cmpnd - da_surge_wdwmax) / da_cmpnd_std)\n",
    "cfi_runoff\n",
    "\n",
    "threshold = xs.nanpercentile(da_cmpnd, q=q, dim='time')\n",
    "ts = xs.peaks_over_threshold(da_cmpnd, min_dist=min_dist, threshold=threshold, dim='time') \n",
    "cfi_surge_peaks = cfi_surge.where(xr.ufuncs.isfinite(ts))\n",
    "cfi_runoff_peaks = cfi_runoff.where(xr.ufuncs.isfinite(ts))\n",
    "\n",
    "fig, ((ax1, ax2, ax3), (ax12, ax22, ax32)) = plt.subplots(2, 3, figsize=(15, 9), sharex=True)\n",
    "sel = dict(time=slice('07-01-2002', '07-01-2003'))\n",
    "ymax = max(ds_i['water_level'].sel(**sel).max().values, ds_i['sea_water_level'].sel(**sel).max().values)\n",
    "ymin = min(ds_i['water_level'].sel(**sel).min().values, ds_i['sea_water_level'].sel(**sel).min().values)\n",
    "ylim = [ymin, ymax]\n",
    "ylab = 'water surface elevation [m+EGM96]'\n",
    "\n",
    "# da_cmpnd.sel(**sel).plot(ax=ax2, c='red')\n",
    "ds_i['water_level'].sel(**sel, scen='surge').plot(ax=ax2, x='time', label='surge', c='blue')\n",
    "ds_i['water_level'].sel(**sel, scen='runoff').plot(ax=ax2, x='time', label='runoff', c='green')\n",
    "ds_i['water_level'].sel(**sel, scen='cmpnd').plot(ax=ax2, x='time', label='compound', c='red')\n",
    "cfi_surge.sel(**sel).plot(ax=ax22, c='blue', linestyle='--')\n",
    "cfi_runoff.sel(**sel).plot(ax=ax22, c='green', linestyle='--')\n",
    "ax2.set_title('river mouth')\n",
    "ax22.set_title('')\n",
    "ax2.set_ylabel(ylab)\n",
    "ax2.set_ylim(ylim)\n",
    "ax2.legend()\n",
    "\n",
    "\n",
    "ds_i['river_discharge'].sel(**sel).plot(ax=ax3, c='green')\n",
    "ds_i['river_discharge_climatology'].sel(**sel).plot(ax=ax3, c='green', linestyle='--')\n",
    "\n",
    "ax3.set_title('upstream boundary')\n",
    "ax3.set_ylabel('discharge [m3/s]')\n",
    "\n",
    "ds_i['sea_water_level_climatology'].sel(**sel).plot(ax=ax1, c='blue', linestyle='--')\n",
    "ds_i['sea_water_level'].sel(**sel).plot(ax=ax1, c='blue')\n",
    "ax1.set_title('downstream boundary')\n",
    "ax1.set_ylabel(ylab)\n",
    "ax1.set_ylim(ylim)\n",
    "\n",
    "# ds_i['sea_water_level_climatology'].sel(**sel).plot(ax=ax1, c='magenta', x='time')\n",
    "\n",
    "# if ts.sel(**sel).dropna('time').values.size>1: # needs two dates to plot\n",
    "#     ts.sel(**sel).dropna('time').plot(c='r', marker='*', markersize=9, linewidth=0, ax=ax)\n",
    "#     cfi_runoff_peaks.sel(**sel).dropna('time').plot(c='r', marker='*', markersize=9, linewidth=0, ax=ax2)\n",
    "\n",
    "\n",
    "# cfi_peaks.mean('time').compute().values, ds_i.to_series().unstack(0).tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
