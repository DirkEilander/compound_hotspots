{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from os.path import join, isfile\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../3-postprocess/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r'/scratch/compound_hotspots'\n",
    "ddir = join(root, 'data', '4-postprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. combine rivmouth attributes with rivmth reanalysis stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_attrs_out = join(ddir, 'rivmth_mean_attrs.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_rivmth_ts = join(ddir, 'rivmth_reanalysis.zarr')\n",
    "ds = xr.open_zarr(fn_rivmth_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_csv_coupling = join(root, 'src', '1-prepare', r'cmf_gtsm_75km_update191203.csv')\n",
    "ccols = ['dist', 'dist2coast', 'rivwth', 'uparea', 'rivhgt', 'elevtn', 'gtsm_egm_offset', \n",
    "         'elvp10', 'elvp90', 'elvavg', 'mean_drain_length', 'mean_drain_slope']\n",
    "coupling = pd.read_csv(fn_csv_coupling, index_col='index')[ccols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data\n",
    "ds_drivers = ds.drop('WSE').mean('ensemble').sel(scen='surge').drop('scen')\n",
    "ds_drivers['Htiderange'] = (ds_drivers['Htide_day_max'] - ds_drivers['Htide_day_min'])\n",
    "ds_drivers_amax = ds_drivers[['Q', 'Hseas_day_mean', 'Hsurge_day_max', 'Hskewsurge_day', 'Htiderange']].resample(time='A').max().mean('time')\n",
    "ds_drivers_amax = ds_drivers_amax.rename({n: '{}_amax'.format(n.split('_')[0]) for n in ds_drivers_amax.data_vars.keys()})\n",
    "ds_drivers_amin = ds_drivers[['Hseas_day_mean']].resample(time='A').min().mean('time')\n",
    "ds_drivers_amin = ds_drivers_amin.rename({n: '{}_amin'.format(n.split('_')[0]) for n in ds_drivers_amin.data_vars.keys()})\n",
    "ds_drivers_mean = ds_drivers[['Q', 'Htiderange']].mean('time')\n",
    "ds_drivers_mean = ds_drivers_mean.rename({n: '{}_mean'.format(n.split('_')[0]) for n in ds_drivers_mean.data_vars.keys()})\n",
    "\n",
    "da_wse = ds['WSE'].sel(scen='surge').drop('scen')\n",
    "da_wse_std = da_wse.std('time').reset_coords(drop=True).mean('ensemble')\n",
    "da_wse_std.name = 'wse_std'\n",
    "\n",
    "attrs = xr.merge([\n",
    "    coupling[ccols].to_xarray(),\n",
    "    ds_drivers_amax,\n",
    "    ds_drivers_amin,\n",
    "    ds_drivers_mean,\n",
    "    da_wse_std\n",
    "]).astype(np.float32).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Q annual max properties\n",
    "da_Q = ds.sel(scen='msl')['Q']\n",
    "da_Q_am = da_Q.groupby('time.year').max('time')\n",
    "da_Q_am_avg = da_Q_am.mean('year')\n",
    "da_Q_am_std = da_Q_am.std('year')\n",
    "da_Q_am_cv = da_Q_am_std / da_Q_am_avg\n",
    "attrs['Q_amax'] = da_Q_am_avg.mean('ensemble')\n",
    "attrs['Q_amax_cv'] = da_Q_am_cv.mean('ensemble')\n",
    "\n",
    "# long term average Q\n",
    "attrs['Q_mean'] = da_Q.mean('time').mean('ensemble')\n",
    "\n",
    "# add Q and Hss annual max properties\n",
    "da_Hskewsurge = ds['Hskewsurge_day']\n",
    "da_Hskewsurge_am = da_Hskewsurge.groupby('time.year').max('time')\n",
    "attrs['Hskewsurge_amax'] = da_Hskewsurge_am.mean('year')\n",
    "attrs['Hskewsurge_amax_cv'] = da_Hskewsurge_am.std('year') / attrs['Hskewsurge_amax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = attrs[attrs.index!=3354] # drop weird outlier ? incorrect coupled ?\n",
    "attrs.to_xarray().to_netcdf(fn_attrs_out)\n",
    "attrs.to_csv(fn_attrs_out.replace('.nc','.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. compute ensemble mean / significance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read rp, peaks and impact results\n",
    "model='mean'\n",
    "attrs = xr.open_dataset(fn_attrs_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dominant driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdw = 1\n",
    "fn_drivers = join(ddir, f'rivmth_drivers_wdw{wdw}.nc')\n",
    "ds_drivers = xr.open_dataset(fn_drivers).sel(index=attrs.index)#.sel(ensemble=[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "def spearmanr(da0, da1, dim='time'):\n",
    "    def _spearmanr(a, b):\n",
    "        return np.asarray(scipy.stats.spearmanr(a,b))\n",
    "    # apply_ufunc parameters\n",
    "    kwargs = dict(               \n",
    "        input_core_dims=[[dim], [dim]], \n",
    "        output_core_dims=[['stats']],\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float],    \n",
    "        output_sizes={'stats': 2}, # on output, <dim> is reduced to length q.size \n",
    "        vectorize=True\n",
    "    )\n",
    "    da_out = xr.apply_ufunc(_spearmanr, da0, da1, **kwargs)\n",
    "    da_out['stats'] = xr.Variable('stats', ['r', 'p'])\n",
    "    return da_out.sel(stats='r').drop('stats'), da_out.sel(stats='p').drop('stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers = ['Hskewsurge','Q']\n",
    "\n",
    "ds_spear = ds_drivers.coords.to_dataset().drop(['year'])\n",
    "for d in drivers:\n",
    "    ds_spear[f'{d}_r'], ds_spear[f'{d}_p'] = spearmanr(ds_drivers['h'], ds_drivers[d], dim='year')\n",
    "\n",
    "ds_spear['H'] = np.logical_and(ds_spear[f'{drivers[0]}_r']>=0.0, ds_spear[f'{drivers[0]}_p']<=0.05)\n",
    "ds_spear['Q'] = np.logical_and(ds_spear[f'{drivers[1]}_r']>=0.0, ds_spear[f'{drivers[1]}_p']<=0.05)\n",
    "ds_spear['insign'] = np.logical_and(~ds_spear['H'], ~ds_spear['Q'])\n",
    "ds_spear['compound'] = np.logical_and(ds_spear['H'], ds_spear['Q'])\n",
    "\n",
    "N = ds_spear['ensemble'].size\n",
    "N2 = int(np.ceil(N/2))\n",
    "Hsign1 = ds_spear['H'].sum('ensemble') >= N2\n",
    "Qsign1 = ds_spear['Q'].sum('ensemble') >= N2\n",
    "compound1 = ds_spear['compound'].sum('ensemble') >= N2\n",
    "insign1 = ds_spear['insign'].sum('ensemble') >= N2\n",
    "\n",
    "ds_corr_stats = ds_drivers.coords.to_dataset().drop(['year', 'ensemble'])\n",
    "ds_corr_stats['driver_H_sign'] = Hsign1\n",
    "ds_corr_stats['driver_Q_sign'] = Qsign1\n",
    "ds_corr_stats['driver_H_r'] = ds_spear[f'{drivers[0]}_r'].mean('ensemble')\n",
    "ds_corr_stats['driver_Q_r'] = ds_spear[f'{drivers[1]}_r'].mean('ensemble')\n",
    "# if N>1:\n",
    "#     ds_corr_stats[f'driver_compound_(N={N})'] = ds_spear['compound'].sum('ensemble') >= N\n",
    "ds_corr_stats[f'driver_compound'] = compound1\n",
    "ds_corr_stats['driver_H'] = np.logical_and(np.logical_and(Hsign1, ~Qsign1), ~compound1)\n",
    "ds_corr_stats['driver_Q'] = np.logical_and(np.logical_and(Qsign1, ~Hsign1), ~compound1)\n",
    "ds_corr_stats['driver_insign'] = np.logical_and(np.logical_and(ds_corr_stats['driver_Q'], ~ds_corr_stats['driver_H']), ~compound1)\n",
    "\n",
    "corr_sum = (ds_corr_stats.drop(['driver_H_r', 'driver_Q_r', 'driver_H_sign', 'driver_Q_sign']).sum()/ds_corr_stats.index.size*100).expand_dims('index').to_dataframe()\n",
    "print(corr_sum.values.sum())\n",
    "corr_sum.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to file \n",
    "fn_spear_out = join(ddir, f'rivmth_drivers_wdw{wdw}_spearmanrank.nc')\n",
    "ds_spear.to_netcdf(fn_spear_out)\n",
    "\n",
    "fn_drivers_out = join(ddir, f'rivmth_drivers_wdw{wdw}_ensemble-{model}.nc')\n",
    "ds_corr_stats.to_netcdf(fn_drivers_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### > data drivers correlations section 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlst = [f'compound', 'H', 'Q', 'insign'] # drivers\n",
    "da_d = xr.concat([ds_spear[[f'{d}_r' for d in drivers]].where(ds_spear[d]).mean('index') for d in dlst], dim='driver').rename({f'{drivers[0]}_r': 'H_r' })\n",
    "da_d['driver'] = xr.Variable('driver', [d for d in dlst])\n",
    "da_d_all = xr.concat([ds_corr_stats[['driver_H_r', 'driver_Q_r']].where(ds_corr_stats[f'driver_{d}']).mean('index') for d in dlst], dim='driver').expand_dims('ensemble')\n",
    "da_d_all = da_d_all.rename({v:v.replace('driver_','') for v in da_d_all.data_vars.keys()})\n",
    "da_d_all['driver'] = xr.Variable('driver', [d for d in dlst])\n",
    "da_d_all['ensemble'] = xr.Variable('ensemble', ['_N3'])\n",
    "xr.concat([da_d, da_d_all], dim='ensemble').to_dataframe().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_d = xr.concat([ds_spear[f'{d}'].where(~ds_spear['compound']).sum('index') if d != 'compound' else ds_spear[f'{d}'].sum('index')\n",
    "                  for d in dlst], dim='driver')\n",
    "da_d_all = xr.concat([ds_corr_stats[f'driver_{d}'].sum('index') for d in dlst], dim='driver').expand_dims('ensemble')\n",
    "da_d_all['driver'] = xr.Variable('driver', [d for d in dlst])\n",
    "da_d_all['ensemble'] = xr.Variable('ensemble', ['_N3'])\n",
    "xr.concat([da_d, da_d_all], dim='ensemble').to_dataframe().unstack() / ds_spear.index.size * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scenario analysis for extreme value statistics \n",
    "for water surface elevation at river mouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_wse_ev = join(ddir, f'rivmth_wse_ev.nc')\n",
    "ds_rp = xr.open_dataset(fn_wse_ev).sel(index=attrs.index)#.sel(ensemble=[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in rp\n",
    "alpha=0.025\n",
    "\n",
    "da_wse_rps_ci = ds_rp['extreme_values_ci']\n",
    "da_wse_rps = ds_rp['extreme_values']\n",
    "    \n",
    "# diff\n",
    "diff_surge_seas = (da_wse_rps.sel(scen='surge') - da_wse_rps.sel(scen='seas'))\n",
    "diff_surge_seas.name = 'diff_h_surge_seas'\n",
    "diff_seas_tide = (da_wse_rps.sel(scen='seas') - da_wse_rps.sel(scen='tide'))\n",
    "diff_seas_tide.name = 'diff_h_seas_tide'\n",
    "diff_surge_tide = (da_wse_rps.sel(scen='surge') - da_wse_rps.sel(scen='tide'))\n",
    "diff_surge_tide.name = 'diff_h_surge_tide'\n",
    "ds_diff = xr.merge([\n",
    "    diff_surge_seas,\n",
    "    diff_seas_tide,\n",
    "    diff_surge_tide,\n",
    "])\n",
    "\n",
    "dim = 'ensemble'\n",
    "# average and calculate significance based on std error\n",
    "N = ds_diff[dim].size\n",
    "ds_diff_mean = ds_diff.mean(dim)\n",
    "ds_diff_dir =  xr.ufuncs.fabs(xr.ufuncs.sign(ds_diff).sum(dim)) == N\n",
    "# ds_diff_sign = xr.ufuncs.fabs(ds_diff_mean / ds_diff.std(dim)) > (2 / xr.ufuncs.sqrt(N-1))\n",
    "\n",
    "ds_lst = []\n",
    "for var in list(ds_diff.data_vars.keys()):\n",
    "    v2,v3 = var.split('_')[-2:]\n",
    "    da_sign = xr.where(\n",
    "        xr.ufuncs.sign(ds_diff[var])>0,\n",
    "        da_wse_rps_ci.sel(scen=v2, alpha=alpha) > da_wse_rps_ci.sel(scen=v3, alpha=1-alpha),\n",
    "        da_wse_rps_ci.sel(scen=v3, alpha=alpha) > da_wse_rps_ci.sel(scen=v2, alpha=1-alpha),\n",
    "    )#.drop('alpha')\n",
    "    da_sign.name = var\n",
    "    ds_lst.append(da_sign)\n",
    "ds_diff_sign2 = xr.merge(ds_lst).sum('ensemble') >= N\n",
    "\n",
    "ds_diff_h_stats = xr.merge([\n",
    "    ds_diff_mean,\n",
    "    ds_diff_dir.rename({v:f'{v}_sign' for v in ds_diff_dir.data_vars.keys()}),\n",
    "    np.logical_and(ds_diff_sign2, ds_diff_dir).rename({v:f'{v}_sign_ci' for v in ds_diff_dir.data_vars.keys()}),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to file \n",
    "fn_wse_ev_out = join(ddir, f'rivmth_wse_ev_ensemble-{model}.nc')\n",
    "ds_diff_h_stats.to_netcdf(fn_wse_ev_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### > data $\\Delta$h section 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input to e.g. TABLE 3 in manuscript\n",
    "rp = 10 # return period\n",
    "df = ds_diff_h_stats.sel(T=rp).to_dataframe()\n",
    "param = 'h'\n",
    "scen = 'surge_tide' # total surge\n",
    "scen = 'surge_seas' # daily\n",
    "scen = 'seas_tide' # seasonal\n",
    "\n",
    "df['diff_class'] = np.logical_and(df[f'diff_{param}_{scen}_sign'], df[f'diff_{param}_{scen}']>0) * 1 +\\\n",
    "                    np.logical_and(df[f'diff_{param}_{scen}_sign'], df[f'diff_{param}_{scen}']<0) * 2\n",
    "df1 = df[['diff_class', f'diff_{param}_{scen}']] #, 'diff_h_surge_tide', 'Q_amax', 'Hskewsurge_amax', 'Hsurge_amax', 'Hseasrange', 'Htiderange_amax']]\n",
    "print(df1[f'diff_{param}_{scen}'].mean())\n",
    "print(df1.groupby('diff_class')['diff_class'].count()/df.index.size*100)\n",
    "df1.groupby('diff_class').mean() #g1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Delta$H per dominant driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_stats = xr.merge([ds_diff_h_stats, \n",
    "                     ds_corr_stats]).sel(T=rp)\n",
    "\n",
    "df = ds_stats.to_dataframe()\n",
    "df['driver'] = df['driver_H']+df['driver_Q']*2+df[f'driver_compound']*3+df[f'driver_insign']*4\n",
    "df1 = df[df['diff_h_surge_tide_sign']==1]\n",
    "df[['driver', 'diff_h_surge_tide', 'diff_h_surge_seas', 'diff_h_seas_tide']].groupby('driver').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flood impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_impact = join(ddir, 'rivmth_pop_affected.nc')\n",
    "ds_impact = xr.open_dataset(fn_impact).sel(index=attrs.index)#.sel(ensemble=[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only locations with significant dH\n",
    "sign_dH = (ds_diff_h_stats['diff_h_surge_tide_sign']>=1).sel(T=ds_impact['T'])\n",
    "\n",
    "# integrate using trapeziodal rule to get expected annual mean\n",
    "T0 = xr.DataArray(\n",
    "    dims=('ensemble', 'T', 'index'), \n",
    "    coords={'ensemble':ds_impact.ensemble, 'T': [1], 'index': ds_impact.index}, \n",
    "    data=np.zeros((ds_impact.ensemble.size,1,ds_impact.index.size))\n",
    ")\n",
    "pop_affected_dH_dp = xr.concat([T0, ds_impact['people_affected_dH'].where(sign_dH)], dim='T')\n",
    "pop_affected_dH_dp['p'] = xr.Variable('T', 1/pop_affected_dH_dp['T'].values)\n",
    "pop_affected_dH_dp = pop_affected_dH_dp.sel(T=pop_affected_dH_dp['T'].values[::-1]).swap_dims({'T':'p'}) \n",
    "ds_impact['people_affected_dH_dp'] = pop_affected_dH_dp.integrate('p')\n",
    "pop_affected_all_dp = xr.concat([T0, ds_impact['people_affected_all']], dim='T')\n",
    "pop_affected_all_dp['p'] = xr.Variable('T', 1/pop_affected_all_dp['T'].values)\n",
    "pop_affected_all_dp = pop_affected_all_dp.sel(T=pop_affected_all_dp['T'].values[::-1]).swap_dims({'T':'p'}) \n",
    "ds_impact['people_affected_all_dp'] = pop_affected_all_dp.integrate('p')\n",
    "ds_impact_stats = ds_impact.mean('ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_impact_out = join(ddir, f'rivmth_pop_affected_ensemble-{model}.nc')\n",
    "ds_impact_stats.to_netcdf(fn_impact_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### > data population exposed section 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_impact['people_affected_all'].where(sign_dH).sum('index').to_dataframe().unstack()/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['people_affected_dH_dp', 'people_affected_all_dp']\n",
    "df = ds_impact[cols].sum('index').to_dataframe()\n",
    "for c in cols:\n",
    "    df.loc['mean',c] = ds_impact_stats[c].sum('index').values\n",
    "df = df/1e6\n",
    "df['perc'] = df['people_affected_dH_dp'] / df['people_affected_all_dp'] * 100\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
